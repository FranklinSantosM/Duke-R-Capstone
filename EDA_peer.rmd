---
title: "Peer Assessment I"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---


First, let us load the data and necessary packages:

```{r load, message = FALSE}
load("ames_train.Rdata")
library(MASS)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(knitr)
```

#
Make a labeled histogram (with 30 bins) of the ages of the houses in the data set, and describe the distribution.


```{r Q1}
# type your code for Question 1 here, and Knit
data <- ames_train

data$age <- sapply(data$Year.Built, function(x) 2017-x)

ggplot(data = data, aes(x = age, y = ..density..)) +
  geom_histogram(bins = 30) +
  geom_density(size = 1) +
  labs(title = "Ages of houses in years", x = 'age', y = "Frequency")

summary.age <- data %>% summarise(mean_age = mean(age),
                   median_age = median(age),
                   sd_age = sd(age),
                   min_age = min(age),
                   max_age = max(age),
                   IQR_age = IQR(age),
                   total = n())
summary.age
```


* * *

The distribution of the calculated age of homes in the sample shows multimodal behavior and a right skewed distribution. The mean age of the house is almost 45 years


* * *


#
The mantra in real estate is "Location, Location, Location!" Make a graphical display that relates a home price to its neighborhood in Ames, Iowa. Which summary statistics are most appropriate to use for determining the most expensive, least expensive, and most heterogeneous (having the most variation in housing price) neighborhoods? Report which neighborhoods these are based on the summary statistics of your choice. Report the value of your chosen summary statistics for these neighborhoods.


```{r Q2}
# type your code for Question 2 here, and Knit
price.neigborhood <- data %>% 
  select(price, Neighborhood)%>%
  group_by(Neighborhood) %>% 
  summarise(mean_price = mean(price),
              median_price = median(price),
              min_price = min(price),
               max_price = max(price),
               IQR_price = IQR(price),
              sd_price  = sd(price),
             var_price = var(price),
            total = n())
# find the required information in the summary data 
ExpNeighborhood <- price.neigborhood[which(price.neigborhood$median_price == max(price.neigborhood$median_price)),]
LeastExpNeighborhood <- price.neigborhood[which(price.neigborhood$median_price == min(price.neigborhood$median_price)),]
HetNeighborhood <- price.neigborhood[which(price.neigborhood$sd_price == max(price.neigborhood$sd_price)),]

# create box plot
ggplot(data, aes(x = Neighborhood, y = (data$price / 100))) +
       geom_boxplot() + 
       labs(title = "Housing prices per Neighborhood", x = 'Neighborhood', y = "Price in [$ 1000s]") +
       theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```


* * *

The boxplot shows the price distribution per neighborhood. The most expensive neighborhood is StoneBr. This was determined by calculating the maximum median value of all houses in a neighborhood, while the least expensive beighborhood is MeadowV. This was determined by calculating the minimum median value of all houses in a Neighborhood. The most heterogeneous neighborhood, measured by looking at the standard deviation of housing prices is StoneBr.



* * *

# 

Which variable has the largest number of missing values? Explain why it makes sense that there are so many missing values for this variable.

```{r Q3}
# type your code for Question 3 here, and Knit
na_count <- sapply(data, function(x) sum(is.na(x)))
df.na_count <- data.frame(na_count)

df.merged <- cbind(c(row.names(df.na_count)), df.na_count[,1])
colnames(df.merged) <- c('feature', 'No_NA')

df.merged[which(df.na_count == max(df.na_count)),]
```


* * *

Pool Quality (Pool.QC) meaning the house with no pool has the higest number of NA’s.

As Iowa has harsh winters, It is expected to have many homes in a given city without a pool.


* * *

#

We want to predict the natural log of the home prices. Candidate explanatory variables are lot size in square feet (Lot.Area), slope of property (Land.Slope), original construction date (Year.Built), remodel date (Year.Remod.Add), and the number of bedrooms above grade (Bedroom.AbvGr). Pick a model selection or model averaging method covered in the Specialization, and describe how this method works. Then, use this method to find the best multiple regression model for predicting the natural log of the home prices.


```{r Q4}
# type your code for Question 4 here, and Knit
full.model <- data %>% 
 select( Lot.Area, Land.Slope, Year.Built, Year.Remod.Add, Bedroom.AbvGr, price)

# remove NAs
full.model <- full.model[complete.cases(full.model), ]
formula <- as.formula(log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)


lm.houses <- lm(formula, full.model)
summary(lm.houses)
```

* * *

The R2 is a tool to evaluate the usefulness of predictors in model development.strength of the model. We performed stepwise method to find the model with the beste R2 value. For that, we start with a full model (containing all predictors), droping one predictor at a time until adjusted R2 value is maximzied.

The full Model uses all canidate features.

log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr

The analysis showes that the full model gave the highest adjusted R2 value.


* * *

```{r}
formula1 <- as.formula(log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
formula2 <- as.formula(log(price) ~ Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
formula3 <- as.formula(log(price) ~ Lot.Area + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
formula4 <- as.formula(log(price) ~ Lot.Area + Land.Slope + Year.Remod.Add + Bedroom.AbvGr)

lm.houses1 <- lm(formula1, full.model)
lm.houses2 <- lm(formula2, full.model)
lm.houses3 <- lm(formula3, full.model)
lm.houses4 <- lm(formula4, full.model)

m1 <- summary(lm.houses1)$adj.r.squared
m2 <- summary(lm.houses2)$adj.r.squared
m3 <- summary(lm.houses3)$adj.r.squared
m4 <- summary(lm.houses4)$adj.r.squared

R_Squared <- rbind(m1, m2, m3, m4)
model <- c('lm.houses1', 'lm.houses2', 'lm.houses3', 'lm.houses4')
df <- data.frame(cbind(model, R_Squared))

colnames(df) <- c('Model', 'R_Squared')

df
```

We want to try the stepwise with the eliminatoon of one predictor at a time,and when we did it we found that the first model with all the variables i.e the full model is the best one.

In order to evaluate the model for regression, the following condition need to be chekced.

1. There is a linear relationship between any numerical predictor variables and the response variable.
2. The residuals are nearly normally distributed
3. The residuals display constant variability
4. The residuals are independent

```{r}
par(mfrow = c(1,3))
hist(lm.houses$residuals)
qqnorm(lm.houses$residuals)
qqline(lm.houses$residuals)
ggplot(data = NULL, aes(x = log(full.model$price), y = lm.houses$residuals)) + 
      geom_point() + geom_hline(yintercept = 0, linetype = 'dashed') + 
      ylab('Residuals') + xlab('Price') 
```

The results of the histogram of the residuals shows a normal distribution around 0, which is slightly left skewed. The Q-Q plot also indicates some skewness in the tails, but there are no major deviations. We can conclude that the conditions for this model are reasonable.

It showed that the residuals are scattered randomly around 0.

```{r}
par(mfrow = c(1,2))
ggplot(data = NULL, aes(x = lm.houses$fitted, y = lm.houses$residuals)) + geom_point() + 
      geom_hline(yintercept = 0, linetype = 'dashed', color = 'red')
```

The results show that the residuals are equally variable for low and high values of the predicted values, i.e., residuals have a constant variability.

#

Which home has the largest squared residual in the previous analysis (Question 4)? Looking at all the variables in the data set, can you explain why this home stands out from the rest (what factors contribute to the high squared residual and why are those factors relevant)?


```{r Q5}
# type your code for Question 5 here, and Knit
summary(lm.houses)
which(abs(resid(lm.houses)) == max(abs(resid(lm.houses))))
full.model.pred <- full.model
full.model.pred <- as.data.frame(full.model.pred)

full.model.pred$predicted <- exp(predict(lm.houses))
full.model.pred$residuals <- residuals(lm.houses)

full.model.pred[428, ]
```

* * *

The house number 428 has the largest squared residual so it stands out.Because of the actual price of 12,789. Our predicted price was 103,176. The sub-average quality of the home may explain the delta between prediciated and actual price. Based on this, the house might be either fair or poor quality and overall quality and condition are also poor.

Adding some of the before mentioned variables (not included in the full model,) to a new model may better predict the sales price of the home.


* * *

#

Use the same model selection method you chose in Question 4 to again find the best multiple regression model to predict the natural log of home prices, but this time **replacing Lot.Area with log(Lot.Area)**. Do you arrive at a model including the same set of predictors?


```{r Q6}
# type your code for Question 6 here, and Knit
full.model <- data %>% 
  select( Lot.Area, Land.Slope, Year.Built, Year.Remod.Add, Bedroom.AbvGr, price)

# make sure no NA values are in the dataset
full.model <- full.model[complete.cases(full.model), ]

formula <- as.formula(log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)

lm.houses.log <- lm(formula, full.model)
summary(lm.houses.log)

formula.Q6_2 <- as.formula(log(price) ~ log(Lot.Area) + Year.Built + Year.Remod.Add + Bedroom.AbvGr)

lm.houses.red <- lm(formula.Q6_2, full.model)
summary(lm.houses.red)
```

* * *
Removing the ‘insignificant’ feature Land.Slope and running the analysis again would lead to a slightly smaller adjusted R2 value of 0.6031. Even with log transform, the model in the question 6 is same as question 4.
* * *

#

Do you think it is better to log transform Lot.Area, in terms of assumptions for linear regression? Make graphs of the predicted values of log home price versus the true values of log home price for the regression models selected for Lot.Area and log(Lot.Area). Referencing these two plots, provide a written support that includes a quantitative justification for your answer in the first part of question 7.

```{r Q7}
# type your code for Question 7 here, and Knit
ames_train.lm <- lm(log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr, data = ames_train)
ames_train$fitted <- fitted(ames_train.lm)
p1 <- ggplot(ames_train) +
  geom_point(aes(y=fitted, x=log(price))) +
  geom_smooth(data=ames_train, aes(y=fitted, x=log(price)), 
              method = "lm", se=FALSE) +
  ggtitle("Fit vs Obsrv: Lot.Area") +
  xlab("Observed") +
  ylab("Fit Value")
#qqnorm(wordrecall$rstudent) 
#qqline(wordrecall$rstudent)
ames_train.lm2 <- lm(log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr, data = ames_train)
ames_train$fitted2 <- fitted(ames_train.lm2)
p2 <- ggplot(ames_train) +
  geom_point(aes(y=fitted2, x=log(price))) +
  geom_smooth(data=ames_train, aes(y=fitted2, x=log(price)), 
              method = "lm", se=FALSE) +
  ggtitle("Fit vs Obsrv: log(Lot.Area)") +
  xlab("Observed") +
  ylab("Fit Value")
grid.arrange(p1, p2, ncol=2)
```

* * *

The Log model version of the model has a considerably large improvement in the Adjusted R2 value versus the No Log model. It is better to log transform Lot.Area. The linear regression model results are based on the assumption of constant variance. The residuals have significantly more larger values at the higher levels of observed values. By log-transforming Lot.aREA, thee residuals are reduced and the constant variance condition is satisfied. The adjusted R-square increases from 0.5598 in the non-log-transformed to model to 0.6032 in the log-transformed model. 


* * *
###